---
title: "Spatial-based clustering and optimization"
author: "`r Sys.getenv('R_NAME')`"
date: "`r strftime(Sys.time(), '%B %e, %Y')`"
knit: (function(inputFile, encoding) rutils::render_doc(inputFile))
output:
  bookdown::html_document2:
    code_folding: show
    theme: united
    highlight: haddock
    toc: true
    toc_float: 
      smooth_scroll: false
      collapsed: true
    number_sections: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, out.width = "100%", fig.height = 6)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

# Introduction

In this document we will perform a basic exploration of possible techniques for 
problems of spatial-based clustering and optimization.

The general context we will consider is where we have events generated by a spatial
point process, and we want to cluster events based on the spatial structure.

The example we will consider for illustrative purposes is wildfire locations. Hypothetically,
we will suppose we want to split up territory into different response teams or groups.
To simplify, we will take several years of wildfire data and assume the event is a point
occurrence at the centroid of the polygon boundary. Keep in mind that we will be considering
a simplified representation and question/problem to facilitate the demonstration of the 
techniques we wish to exhibit.

This type of analysis could be applied to a problem like defining and assigning
claim handling territories.

# Map of centroids of MTBS wildfire locations from 2010 to 2018

We will use data on large wildfire from the US government interagency program
[Monitoring Trends in Burn Severity](https://www.mtbs.gov/project-overview).

We will filter to wildfires between 2010 and 2018 occurring within a 50 kilometer
buffer of the Western United States (the states of Washington, Oregon, California,
Nevada, Arizona, Utah, Idaho, Montana, Wyoming, Colorado, and New Mexico).

```{r}
library(dplyr)
library(leaflet)
library(purrr)
library(sf)

aea_proj <- glue::glue(
  "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 ",
  "+x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=km"
)
```

```{r}
mtbs_perimeter <- read_sf("data/mtbs_perims_DD")

states_western <- c("WA", "OR", "CA", "NV", "AZ", "UT", "ID", "MT", "WY", "CO", "NM")

region <- sfcensus::state_sf %>% 
  filter(state_abb %in% states_western) %>% 
  st_transform(aea_proj) %>% 
  st_union() %>% 
  st_buffer(50) %>%
  st_transform(4326)

mtbs_region <- mtbs_perimeter %>%
  filter(Fire_Type == "Wildfire", Year %in% 2010:2018) %>%
  st_transform(4326) %>%
  st_join(st_sf(region), left = FALSE)

fire_centroids <- mtbs_region %>%
  st_transform(aea_proj) %>%
  st_centroid() %>%
  mutate(
    txt = glue::glue(
      "Fire Name: {Fire_Name}<br>",
      "Start date: {StartMonth}/{StartDay}/{Year}<br>",
      "Acres: {formatC(Acres, format = 'f', big.mark = ',', digits = 0)}"
    ),
    txt = purrr::map(txt, htmltools::HTML)
  ) 
```

The following map shows the centroids of this filtered subset of wildfires. 
When hovering over a point, there is a tooltip indicating the fire name,
start date, and number of acres burned.

```{r}
leaflet() %>%
  addProviderTiles("CartoDB.Voyager") %>%
  addCircleMarkers(
    data = st_transform(fire_centroids, 4326),
    weight = 0.5,
    radius = 4,
    label = ~txt,
    labelOptions = labelOptions(textsize = "14px")
  )
```

<br>

# Iterative splitting of hierarchical clusters

The first technique we will apply to interrogate the spatial structure of the
wildfire location data is an approach based on hierarchical clustering.

We will recursively split clusters into two until the maximum cluster size is below 
a threshold, in this example 8% of the total number of points.

Cluster sizes may and do vary substantially, but this insight into the concentrations
and geographic patterns of occurrence.

```{r}
hclust_threshold <- nrow(fire_centroids) * 0.08

wildfire_clust <- fire_centroids %>%
  st_transform(aea_proj) %>%
  mutate(cluster = 1)

while (TRUE) {
  wildfire_split <- wildfire_clust %>% 
    group_split(cluster)
  if (!any(purrr::map_lgl(wildfire_split, ~nrow(.) > hclust_threshold))) break
  
  wildfire_split_next <- wildfire_split %>%
    purrr::modify_if(
      ~nrow(.) > hclust_threshold,
      function(pts) {
        hc <- hclust(dist(st_coordinates(pts)), method = "ward.D2")
        hc_split <- cutree(hc, k = 2)
        mutate(pts, cluster = paste0(cluster, hc_split))
      }
    )
  
  wildfire_clust <- st_as_sf(data.table::rbindlist(wildfire_split_next))
}
```

```{r}
wildfire_clust_numbers <- tibble(cluster = unique(wildfire_clust$cluster)) %>%
  mutate(cluster_num = forcats::fct_inorder(paste("Cluster", 1:n())))

wildfire_clust_num <- wildfire_clust %>%
  left_join(wildfire_clust_numbers, by = "cluster")

clusters <- unique(wildfire_clust_num$cluster_num)

clust_pal <- colorFactor(pals::glasbey(length(clusters)), domain = clusters)

leaflet(data = st_transform(wildfire_clust_num, 4326)) %>%
  addProviderTiles("CartoDB.Voyager") %>%
  addCircleMarkers(
    color = ~clust_pal(cluster_num),
    fillColor = ~clust_pal(cluster_num),
    weight = 0.5,
    radius = 4
  ) %>%
  addLegend("bottomright", clust_pal, clusters, opacity = 0.9)
```

# Assignment problem to construct equal-sized groups

Next, let's consider a scenario where we want to split the set of wildfire locations
into a set of equally-sized groups. As an example, perhaps we want to set up a
collection of response teams which will be responsible for responding to and handling
wildfires in different regions. Suppose we want each team to have as geographically 
compact a region assigned to them as possible, and we want each team to have an 
approximately even workload. We will assume the distribution of wildfires over the 
recent several years in our data set accurately reflects our expectation for the 
future distribution of the intensity of the wildfire point process, and that 
wildfire events are independent. These assumption may not and in fact probably do
not hold, but they serve our purpose for this demonstration.

With this setup, we will cast this as an 
[assignment problem](https://en.wikipedia.org/wiki/Assignment_problem) 
type of combinatorial optimization problem.

We will assume we want to assign wildfires to ten groups of equal size, 
where the sum of squared distances between wildfire points and their group
centroids is minimized.

We will use the `solve_LSAP` function in the [clue](https://cran.r-project.org/package=clue)
package to do this. The `solve_LSAP` uses the efficient Hungarian method
to solve a linear assignment problem.

We will perform this optimization in an iterative procedure. First, we will 
get initial centers by taking the centers from a k-means clustering with ten clusters.
We then get a distance matrix of the distances between each wildfire point and the ten centers.
We then construct a cost matrix to use in the assignment problem by replicating 
copies of the squared distance matrix (in a columnwise direction, that is, 
binding copies of the distance matrix to itself as additional columns) 
until there are at least as many columns as rows.

This is the cost matrix provided to the assignment problem solver. Each row (a wildfire point)
gets assigned to one of the columns (corresponding to one of the centers), minimizing
the total cost.
Virtually equally column gets one wildfire assigned to it (only if there are a few extra columns
if the number of groups does not evenly divide the number of points will there be a few leftovers),
and there are an equal number of columns associated with each center, so an approximately
equal number of points are assigned to each group.

We then compute the centers of this new assignment and feed these centers into the algorithm
for another assignment optimization. In this example, we will run through five iterations.

The result after five iterations will be our final assignment of wildfires to groups.

```{r}
n_groups <- 10
n_points <- nrow(fire_centroids)
n_iterations <- 5

centers <- kmeans(st_coordinates(fire_centroids), n_groups)$centers %>%
  as_tibble() %>%
  st_as_sf(coords = c("X", "Y"), crs = aea_proj)

lsap_result <- list()

for (iteration in 1:n_iterations) {
  dist_mat <- st_distance(fire_centroids, centers) %>%
    as.matrix()
  
  cost_mat <- matrix(0, nrow = n_points, ncol = n_groups * ceiling(n_points / n_groups))
  for (mat_copy in 1:ceiling(n_points / n_groups)) {
    cost_mat[, 1:n_groups + (n_groups * (mat_copy - 1))] <- dist_mat ^ 2
  }
  
  message(Sys.time(), " Assignment problem ", iteration)
  lsap_result[[iteration]] <- clue::solve_LSAP(cost_mat)
  
  centers <- fire_centroids %>%
    mutate(cluster = lsap_result[[iteration]] %% n_groups) %>%
    group_by(cluster) %>%
    summarise() %>%
    st_centroid()
}
```

The following map shows the final assignment of wildfire locations to clusters.
The center of each group is displayed as a large point.

```{r}
lsap_clusters <- unique(1 + (lsap_result[[n_iterations]] %% n_groups))

lsap_pal <- colorFactor(pals::glasbey(length(lsap_clusters)), domain = lsap_clusters)

assign_clusters <- fire_centroids %>%
  mutate(cluster = 1 + (lsap_result[[n_iterations]] %% n_groups))

assign_cluster_center <- assign_clusters %>%
  group_by(cluster) %>%
  summarise() %>%
  st_centroid()

leaflet(data = st_transform(assign_clusters, 4326)) %>%
  addProviderTiles("CartoDB.Voyager") %>%
  addCircleMarkers(
    color = ~lsap_pal(cluster),
    fillColor = ~lsap_pal(cluster),
    weight = 1, 
    radius = 4,
    fillOpacity = 0.2
  ) %>%
  addLegend("bottomright", lsap_pal, lsap_clusters, opacity = 0.9) %>%
  addCircleMarkers(
    data = st_transform(assign_cluster_center, 4326),
    color = ~lsap_pal(cluster),
    fillColor = ~lsap_pal(cluster),
    weight = 1,
    radius = 15,
    fillOpacity = 0.8
  )
```

<br>

Note that each cluster/center has an approximately equal number of wildfire points.

```{r}
assign_clusters %>% 
  st_set_geometry(NULL) %>%
  as_tibble() %>% 
  count(cluster)
```


# Further enhancements

There are a number of potential enhancements that could be made to an analysis of this type.
In this example, we just used simple direct Euclidean distance as the distance metric.
If one were attempting to optimizing a problem of driving time, where responders are
driving from the center location to the point occurrence locations, using driving time 
duration would be an improvement.

Using the as-the-crow-flies Euclidean distance doesn't account for considerations for
additional time/cost of driving travel across natural or other barriers, such as bodies
of water (e.g., east and west of the Puget Sound in Washington; north and south of the 
Columbia River between Interstate 5 and the Pacific Ocean in Washington and Oregon;
east and west of Lake Michigan, such as between Milwaukee, WI and Grand Rapids, MI).

It is possible to perform analysis using driving distances and travel times 
using the Open Street Map-based [Open Source Routing Machine](http://project-osrm.org/),
and, for example, the [R interface](https://github.com/rCarto/osrm) to the OSRM.

